##### 聚类 chapter-2
在这章，我们会使用上一章学习到的数据转换操作，从给定的数据集上挖掘一些有趣的模式：或者发现数据分组或者聚类。

本章还会学习两个新工具： 使用scikit-learn来生成人工数据集；使用matplotlib对数据和模型结果进行图形化展示。
本章包含以下内容：
	* 学习聚类如何工作。区分和分类技术的区别
	* 使用scikit-learn和matplotlib扩展数据集，并进行专业的数据可视化。
	* 完成k-means算法
	* 实现最近邻算法，并和k-means作比较
	
#### 非监督学习-从数据中学习
本章学习两类主要的非监督学习.非监督学习可以理解为在数据集中找模式。通常不给定前置信息/先验知识的情况下，自动确定数据中信息的组织形式。并找出不同信息的结构。
### 聚类
对于没有打标的数据集上的一个最简单的操作就是，理解数据集公共特征上的分组。数据集可以被分成任意部分，每个部分由其中的中心节点代表。为了定义将一个组分成多个组的操作，我们需要定义数据之间的距离。我们认为在给定距离定义下，节点离自己组中心节点的距离比里其他组中心节点的距离要小。
下图表示了一个典型的聚类算法的结果，以及其中的聚类中心。
2-pic1
## k-means
k-means是一个流行的算法，也很容易实现。使用它作为处理数据的第一步通常能得到对数据的很好的先验理解。
## k-means机制
kmeans将数据分成不相交的k个分组/类别,使用成员均值作为该组的表示。通常这个值称为centroid/质心，这和数学上的定义相同。该值通常是一个任意维度的向量。
k-means很简单，不需要提前知道聚类的数目，它会自动找到合适的质心。
想要知道聚类数据是多少合适，可以使用`手肘法`
## 算法迭代标准
这个算法的标准和目标是最小化类内数据到类质心的平方和。这个准则也被称为 `minimization of inertia` 
 2-pic2
## 算法分解
2-pic3
k-means可以分解为以下步骤：
 * 从未分类的样本开始，接受参数k，即要分类为k个类，并找出k个数据样本作为初始质心，为了方便可以直接取样本列表的前k个作为初始质心。
 * 计算所有样本到质心的距离，并将样本分配到对应质心对应类，重新计算质心。可以看到质心在移动，并且越来越合理。
 * 随着质心的变化，其他样本到质心的距离也会变化。
 * 重复第一步。直到满足结束条件。
结束条件包括以下几条：
 * 迭代次数N。有些时候可能我们经过很多轮的无意义计算，整个算法收敛的特别慢，质心不稳定，最终导致结果不佳。为了避免过长的迭代过程，设置一个最大迭代次数作为停止条件。
 * 看之前的结果，一个观察算法时候收敛的标准是，查看质心是否稳定。当发现各个类别的元素不再变化的时候，即质心稳定不变的时候，停止算法。
## 优点和缺点
k-means的优点：
* 扩展性好，大部分的计算可以并行运行。
* 应用广泛。
缺点：
* 需要先验知识k。
* 离群点可能会影响质心的位置。（异常值敏感）
* 假设数据是凸的，非凸集合分类不好
* 局部最优
## k-nn
k-nn也是此类算法中的一个入门算法，它通过检查数据节点的邻居，并假设，每个数据节点都属于已知的其他节点的类别。
2-pic4
## k-nn机制
k-nn有很多种实现场景。这里我们考虑半监督算法。我们假设已经有一些标签数据（即已经标注属于哪些类别的数据）之后我们根据这些已经标注的训练数据对全体数据集进行聚类。
k-nn的算法流程可以分解如下：
*  读入训练数据。
*  读取待分类/数据，并计算其与当前其他训练数据的欧式距离。
*  选取top-k近的的训练样本，投票计算出标签。
*  重复上述流程。之后所有样本分类完成。
##优点和缺点：
优点：
	* 简单，不用调参
	* 没有训练过程，训练样本越多，模型越准确
缺点：
	* 计算成本高。
### 常用库
下面我们讨论一些常用的库
## matplotlib 画图库
数据可视化是数据科学中重要的一环，因此我们需要有效的工具来对我们的数据进行可视化。tf中并没有画图相关的工具，因此我们使用matplotlib库。
来自matplotlib官网：https://matplotlib.org，：
`matplotlib 是一个python 2维画图库，可以产出出版级图片质量，丰富的硬拷格式和跨平台交互环境`
# 假数据画图
下面例子会产生100个随机数，图形化展示它们后，保存到图片文件内。

结果如下：
2-pic3
## scikit-learn 数据集
tf目前没有提供生成假数据的工具，因此，我们使用sklearn库中对应功能。
关于scikit-learn库
```scikit-learn（scikit.learn）是一个python语言上的开源机器学习库。包括丰富的分类，回归，聚类算法，并被设计为能和python上的科学计算库numpy和scipy交互使用
```
下面的例子中，我们会使用scikit-learn的数据集模块，来引入一些著名的，特征处理好的数据集。
## 假数据
我们会使用的一些数据集如下：
2-pic4
# blobs数据集
这是个测试聚类算法的理想数据集，这个数据集内在的分成多个组，组间区分明确。
调用方法
生成blob数据集需要调用以下方法。
```
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

if __name__ == "__main__":

    with tf.Session() as sess:
        ax = plt.subplot()
        ax.plot(tf.random_normal([100]).eval(),
                tf.random_normal([100]).eval(),'o')
        ax.set_title('Sample  random  plot for tensorflow')
        plt.show()

```
这里n_samples是样本数，n_features是列数量或者数据特征数量。centers指分类个数。cluster_std指每个分类的标准差，center_box指当分类中心是随机指定的时候，分类中心所处的边界范围。shuffle指是否需要对样本进行洗牌，random_state是随机种子。
# circle 数据集
这个数据集会产生圆圈数据，一个圈套另外一个圈。数据是非线性的，可分的数据集。所以需要非线性模型来解决。这里k-means这种简单算法就不再适用。我们这里使用它来阐释相关概念。
调用方法
生成该数据集调用以下方法。
```
X,y = make_blobs(n_samples=100,n_features=2,centers=3,cluster_std=1.0,center_box=(-10,10),shuffle=True,
           random_state=None)

```
这里n_samples同样表示样本数，shuffle表示是否对样本洗牌，noise指圆圈数据上随机扰动个数。random_state是随机种子，factor表示不同圆圈的大小比例
# 月亮数据集
这是另外一个非线性问题，也是另外一种分类方式，月亮数据集不像圆圈数据集一样数据形成圆圈（更像个月牙）
### 项目1 使用k-means在假数据集上进行聚类。
##数据集描述
本节，我们会使用具有两种特性的生成数据集： 1 数据集的线性可分；2 是否清晰的存在分类。
## 产生数据
使用以下代码，我们会创建所以项目需要用到的数据,并首先对数据进行可视化：
```centers = [(-2,-2),(-2,1.5),(1.5,-2),(2,1.5)]
X,y = make_blobs(n_samples=200,centers=centers,n_features=2,cluster_std=0.8,shuffle=False,random_state=42)
plt.plot(np.array(centers).transpose()[0],np.array(centers).transpose()[1],marker='o',s=250)
plt.show()

```
2-pic4
## 模型架构
如下，数据集中的点由2维坐标表示，存在points变量中，centroid变量表示不同分组的中心坐标。cluster_assignments变量表示，不同点的归属。（点到centroid的映射）
例如，cluster_assignments[2]=1表示，数据点data[2]属于centroid[1 ]所代表的分组。之后我们可以使用matplotlib先画出各个centroid的位置图。
```
```
2-pic5

## 损失函数和优化循环
我们会对centroid复制n份，每个点复制k份，这样点和centroid是形成一个n*k的tensor。~~我们通过可以计算每个点到每个centroid的距离（每个维度）。~~
之后我们对point的所有维度进行求和，计算最小和所在索引，centroid更新使用bucket：mean
函数，参见完整代码。
```
```

## 环形数据上的kmeans
通过对环形数据进行图形化分析，我们可以看到，此类数据很难使用一些简单的均值代表。就像下图展示的，要么两个环共享一个centroid节点，要么它们顾离太近，导致无法预测。
对于这个数据我们只将数据分成两个class，显示出kmeans算法的缺陷即可。你会看到，kmeans算法会使初始的centroid节点收敛到样本中最中间的位置，并将数据线性分开。而这也正是此算法的局限，对于非线性可分的数据，我们可以使用其他统计方法，例如DBSCAN（密度聚类） ， 此书中暂且不提。
###项目2 在构造数据集上使用最近邻算法
这个项目我们会使用

 
 

